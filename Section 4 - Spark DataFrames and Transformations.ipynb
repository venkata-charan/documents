{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2018-01-30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 4: Spark DataFrames & Transformations\n",
    "\n",
    "In this section we will look at the Spark DataFrames and the transformations available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "simple_rdd = sc.parallelize([\n",
    "      ['2017-02-01','Rachel', 19, 156, 'Sydney']\n",
    "    , ['2018-01-01','Albert',  3,  45, 'New York']\n",
    "    , ['2018-03-02','Jack',   61, 190, 'Krakow']\n",
    "    , ['2017-12-31','Skye',    8,  82, 'Harbin']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_df = spark.createDataFrame(\n",
    "    simple_rdd, \n",
    "    ['Date','Name', 'Age', 'Weight', 'Location']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------+--------+\n",
      "|      Date|  Name|Age|Weight|Location|\n",
      "+----------+------+---+------+--------+\n",
      "|2017-02-01|Rachel| 19|   156|  Sydney|\n",
      "|2018-01-01|Albert|  3|    45|New York|\n",
      "|2018-03-02|  Jack| 61|   190|  Krakow|\n",
      "|2017-12-31|  Skye|  8|    82|  Harbin|\n",
      "+----------+------+---+------+--------+"
     ]
    }
   ],
   "source": [
    "simple_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+------+------+\n",
      "|Age|      Date|Location|  Name|Weight|\n",
      "+---+----------+--------+------+------+\n",
      "| 19|2017-02-01|  Sydney|Rachel|   156|\n",
      "|  3|2018-01-01|New York|Albert|    45|\n",
      "| 61|2018-03-02|  Krakow|  Jack|   190|\n",
      "|  8|2017-12-31|  Harbin|  Skye|    82|\n",
      "+---+----------+--------+------+------+"
     ]
    }
   ],
   "source": [
    "json_string = [\n",
    "    '{\"Date\":\"2017-02-01\",\"Name\":\"Rachel\",\"Age\":19,\"Weight\":156,\"Location\":\"Sydney\"}', \n",
    "    '{\"Date\":\"2018-01-01\",\"Name\":\"Albert\",\"Age\":3 ,\"Weight\":45 ,\"Location\":\"New York\"}', \n",
    "    '{\"Date\":\"2018-03-02\",\"Name\":\"Jack\"  ,\"Age\":61,\"Weight\":190,\"Location\":\"Krakow\"}', \n",
    "    '{\"Date\":\"2017-12-31\",\"Name\":\"Skye\"  ,\"Age\":8 ,\"Weight\":82 ,\"Location\":\"Harbin\"}'\n",
    "]\n",
    "\n",
    "simple_df_json = spark.read.json(sc.parallelize(json_string))\n",
    "simple_df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df = spark.read.csv(\n",
    "    '../data/sample_data.csv'\n",
    "    , header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|    1/6/16|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|2017-03-02|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|    2/9/16|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|   2/26/16|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "sample_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame schema\n",
    "### RDDs reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Weight: long (nullable = true)\n",
      " |-- Location: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "simple_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically specifying schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------+--------+\n",
      "|      Date|  Name|Age|Weight|Location|\n",
      "+----------+------+---+------+--------+\n",
      "|2017-02-01|Rachel| 19|   156|  Sydney|\n",
      "|2018-01-01|Albert|  3|    45|New York|\n",
      "|2018-03-02|  Jack| 61|   190|  Krakow|\n",
      "|2017-12-31|  Skye|  8|    82|  Harbin|\n",
      "+----------+------+---+------+--------+"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import datetime as dt\n",
    "\n",
    "schema = [\n",
    "      ('Date', typ.DateType())\n",
    "    , ('Name', typ.StringType())\n",
    "    , ('Age',  typ.IntegerType())\n",
    "    , ('Weight', typ.IntegerType())\n",
    "    , ('Location', typ.StringType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([typ.StructField(e[0], e[1], True) for e in schema])\n",
    "\n",
    "simple_df_schema = spark.createDataFrame(\n",
    "      simple_rdd\n",
    "        .map(lambda row: \n",
    "             [dt.datetime.strptime(row[0], '%Y-%m-%d')] + row[1:]\n",
    "            )\n",
    "    , schema=schema\n",
    ")\n",
    "\n",
    "simple_df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "simple_df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically inferring schema while reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- UnitCost: string (nullable = true)\n",
      " |-- Total: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df_inferred = spark.read.csv(\n",
    "    '../data/sample_data.csv'\n",
    "    , header=True\n",
    "    , inferSchema = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- UnitCost: double (nullable = true)\n",
      " |-- Total: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|      null|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-02-26|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "sample_df_inferred = (\n",
    "    sample_df_inferred\n",
    "    .withColumn('OrderDate'\n",
    "                , f.to_date('OrderDate', 'MM/dd/yy')\n",
    "               )\n",
    ")\n",
    "\n",
    "sample_df_inferred.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .agg(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(Total)|\n",
      "+------------------+\n",
      "|456.46232558139553|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.agg(\n",
    "    {\n",
    "          'Total': 'avg'\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+-----------------+\n",
      "|Total_min|Total_max|         Total_avg|     Total_stddev|\n",
      "+---------+---------+------------------+-----------------+\n",
      "|     9.03|  1879.06|456.46232558139553|447.0221038416717|\n",
      "+---------+---------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "aggregations = [\n",
    "      ('Total', f.min,    'Total_min')\n",
    "    , ('Total', f.max,    'Total_max')\n",
    "    , ('Total', f.avg,    'Total_avg')\n",
    "    , ('Total', f.stddev, 'Total_stddev')\n",
    "]\n",
    "\n",
    "(\n",
    "    sample_df_inferred\n",
    "    .agg(*[e[1](e[0]).alias(e[2]) for e in aggregations])\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .sql(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+-----------------+\n",
      "|Total_min|Total_max|         Total_avg|        Total_std|\n",
      "+---------+---------+------------------+-----------------+\n",
      "|     9.03|  1879.06|456.46232558139553|447.0221038416717|\n",
      "+---------+---------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df_inferred')\n",
    "\n",
    "(\n",
    "    spark\n",
    "    .sql('''\n",
    "        SELECT \n",
    "              MIN(Total)    AS Total_min\n",
    "            , MAX(Total)    AS Total_max\n",
    "            , AVG(Total)    AS Total_avg\n",
    "            , STDDEV(Total) AS Total_std\n",
    "        FROM sample_df_inferred\n",
    "    ''')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Total_min|Total_max|\n",
      "+---------+---------+\n",
      "|     9.03|  1879.06|\n",
      "+---------+---------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .selectExpr(\n",
    "          'MIN(Total) AS Total_min'\n",
    "        , 'MAX(Total) AS Total_max'\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating temporary views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Temporary table 'sample_df_inferred' already exists;\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 153, in createTempView\n",
      "    self._jdf.createTempView(name)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 71, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"Temporary table 'sample_df_inferred' already exists;\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.createTempView('sample_df_inferred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df_inferred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regions = spark.createDataFrame(\n",
    "    sc.parallelize([\n",
    "        ('Central', 'Chicago')\n",
    "        , ('West', 'Seattle')\n",
    "        , ('East', 'Boston')\n",
    "    ]),\n",
    "    ['Region', 'Headquarters']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+------+-----+--------+------+------------+\n",
      "| Region| OrderDate|    Rep|  Item|Units|UnitCost| Total|Headquarters|\n",
      "+-------+----------+-------+------+-----+--------+------+------------+\n",
      "|Central|      null| Kivell|Binder|   50|   19.99| 999.5|     Chicago|\n",
      "|   East|2016-01-06|  Jones|Pencil|   95|    1.99|189.05|      Boston|\n",
      "|Central|2016-02-09|Jardine|Pencil|   36|    4.99|179.64|     Chicago|\n",
      "|Central|2016-02-26|   Gill|   Pen|   27|   19.99|539.73|     Chicago|\n",
      "+-------+----------+-------+------+-----+--------+------+------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred.join(\n",
    "        regions\n",
    "        , on=['Region']\n",
    "        , how='left_outer'\n",
    "    )\n",
    "    .orderBy('OrderDate')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------+------------------+------------------+------------------+\n",
      "|summary| Region|     Rep|  Item|             Units|          UnitCost|             Total|\n",
      "+-------+-------+--------+------+------------------+------------------+------------------+\n",
      "|  count|     43|      43|    43|                43|                43|                43|\n",
      "|   mean|   null|    null|  null|49.325581395348834|20.308604651162792|456.46232558139553|\n",
      "| stddev|   null|    null|  null|30.078247899067208| 47.34511769375187| 447.0221038416717|\n",
      "|    min|Central| Andrews|Binder|                 2|              1.29|              9.03|\n",
      "|    max|   West|Thompson|Pencil|                96|             275.0|           1879.06|\n",
      "+-------+-------+--------+------+------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|             Units|          UnitCost|             Total|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                43|                43|                43|\n",
      "|   mean|49.325581395348834|20.308604651162792|456.46232558139553|\n",
      "| stddev|30.078247899067208| 47.34511769375187| 447.0221038416717|\n",
      "|    min|                 2|              1.29|              9.03|\n",
      "|    max|                96|             275.0|           1879.06|\n",
      "+-------+------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "numeric_columns = [e[0] \n",
    "         for e in sample_df_inferred.dtypes \n",
    "         if e[1] in ('int', 'double')\n",
    "        ]\n",
    "\n",
    "(\n",
    "    sample_df_inferred\n",
    "    .select(numeric_columns)\n",
    "    .describe()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|        mean_Units|     mean_UnitCost|        mean_Total|      stddev_Units|  stddev_UnitCost|     stddev_Total|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|49.325581395348834|20.308604651162792|456.46232558139553|30.078247899067208|47.34511769375187|447.0221038416717|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.agg(*\n",
    "    [f.mean(f.col(e)).alias('mean_' + e) for e in numeric_columns] +\n",
    "    [f.stddev(f.col(e)).alias('stddev_' + e) for e in numeric_columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-------+-----+--------+------+\n",
      "| OrderDate| Region|   Rep|   Item|Units|UnitCost| Total|\n",
      "+----------+-------+------+-------+-----+--------+------+\n",
      "|2016-11-25|Central|Kivell|Pen Set|   96|    4.99|479.04|\n",
      "|2016-09-01|Central| Smith|   Desk|    2|   125.0| 250.0|\n",
      "|2016-10-05|Central|Morgan| Binder|   28|    8.99|251.72|\n",
      "|2017-05-31|Central|  Gill| Binder|   80|    8.99| 719.2|\n",
      "+----------+-------+------+-------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.distinct().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "| Region|     Rep|\n",
      "+-------+--------+\n",
      "|Central| Andrews|\n",
      "|Central|    Gill|\n",
      "|Central| Jardine|\n",
      "|Central|  Kivell|\n",
      "|Central|  Morgan|\n",
      "|Central|   Smith|\n",
      "|   East|  Howard|\n",
      "|   East|   Jones|\n",
      "|   East|  Parent|\n",
      "|   West| Sorvino|\n",
      "|   West|Thompson|\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .select('Region', 'Rep')\n",
    "    .distinct()\n",
    "    .orderBy('Region', 'Rep')\n",
    "    .show()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2018-02-01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 5: Data Processing with Spark DataFrames\n",
    "\n",
    "In this section we will look at processing data using Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|      null|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-02-26|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "sample_df_inferred = spark.read.csv(\n",
    "    '../data/sample_data.csv'\n",
    "    , header=True\n",
    "    , inferSchema = True\n",
    ")\n",
    "\n",
    "sample_df_inferred = (\n",
    "    sample_df_inferred\n",
    "    .withColumn('OrderDate'\n",
    "                , f.to_date('OrderDate', 'MM/dd/yy')\n",
    "               )\n",
    ")\n",
    "\n",
    "sample_df_inferred.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing schema\n",
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+--------+\n",
      "|    Rep|  Item|Units|UnitCost|\n",
      "+-------+------+-----+--------+\n",
      "|  Jones|Pencil|   95|    1.99|\n",
      "| Kivell|Binder|   50|   19.99|\n",
      "|Jardine|Pencil|   36|    4.99|\n",
      "|   Gill|   Pen|   27|   19.99|\n",
      "+-------+------+-----+--------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .drop('OrderDate', 'Region', 'Total')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      Date|Location|\n",
      "+----------+--------+\n",
      "|2016-01-06|    East|\n",
      "|      null| Central|\n",
      "|2016-02-09| Central|\n",
      "|2016-02-26| Central|\n",
      "+----------+--------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .select(\n",
    "        f.col('OrderDate').alias('Date')\n",
    "        , f.col('Region').alias('Location')\n",
    "    )\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+-----+--------+------+\n",
      "|      Date|Location|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+--------+-------+------+-----+--------+------+\n",
      "|2016-01-06|    East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|      null| Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|2016-02-09| Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-02-26| Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+----------+--------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .withColumnRenamed('OrderDate', 'Date')\n",
    "    .withColumnRenamed('Region', 'Location')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-02-26|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "|2016-03-15|   West|Sorvino|Pencil|   56|    2.99|167.44|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_df_broken_rdd = (\n",
    "    sample_df_inferred\n",
    "    .rdd\n",
    "    .map(lambda row: \n",
    "         row[:5] + \n",
    "         ((None, None) if np.random.rand() < 0.2 else tuple(row[5:]))\n",
    "    )\n",
    ")\n",
    "\n",
    "sample_df_broken = (\n",
    "    spark\n",
    "    .createDataFrame(\n",
    "        sample_df_broken_rdd\n",
    "        , sample_df_inferred.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "sample_df_broken.dropna(subset=['OrderDate']).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------------------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost|             Total|\n",
      "+----------+-------+-------+------+-----+--------+------------------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|            189.05|\n",
      "|      null|Central| Kivell|Binder|   50|   19.99| 999.4999999999999|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64000000000001|\n",
      "|2016-02-26|Central|   Gill|   Pen|   27|   19.99| 539.7299999999999|\n",
      "+----------+-------+-------+------+-----+--------+------------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "avg_unitCost = (\n",
    "    sample_df_broken\n",
    "    .select('UnitCost')\n",
    "    .agg(\n",
    "        f.mean(f.col('UnitCost'))\n",
    "        .alias('UnitCost')\n",
    "    ).toPandas()\n",
    "    .to_dict('records')\n",
    ")\n",
    "\n",
    "sample_df_fixed = (\n",
    "    sample_df_broken\n",
    "    .fillna(*avg_unitCost)\n",
    "    .withColumn('Total', f.col('Units') * f.col('UnitCost'))\n",
    ")\n",
    "\n",
    "sample_df_fixed.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-03-15|   West|Sorvino|Pencil|   56|    2.99|167.44|\n",
      "|2016-04-18|Central|Andrews|Pencil|   75|    1.99|149.25|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.where('Item = \"Pencil\"').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-01-06|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|2016-02-09|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2016-03-15|   West|Sorvino|Pencil|   56|    2.99|167.44|\n",
      "|2016-04-18|Central|Andrews|Pencil|   75|    1.99|149.25|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.filter('Item = \"Pencil\"').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating data in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|     Rep| Region|count|\n",
      "+--------+-------+-----+\n",
      "|   Jones|   East|    8|\n",
      "|  Kivell|Central|    4|\n",
      "| Jardine|Central|    5|\n",
      "|  Howard|   East|    2|\n",
      "|  Morgan|Central|    3|\n",
      "| Sorvino|   West|    4|\n",
      "|Thompson|   West|    2|\n",
      "| Andrews|Central|    4|\n",
      "|   Smith|Central|    3|\n",
      "|    Gill|Central|    5|\n",
      "|  Parent|   East|    3|\n",
      "+--------+-------+-----+"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.groupby('Rep', 'Region').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+\n",
      "|   Item|UnitsTotal|        GrandTotal| AvgPerTransaction|\n",
      "+-------+----------+------------------+------------------+\n",
      "|   Desk|        10|            1700.0| 566.6666666666666|\n",
      "| Binder|       722|           9577.65|            638.51|\n",
      "|    Pen|       278|           2045.22|           409.044|\n",
      "|Pen Set|       395|           4169.87| 595.6957142857143|\n",
      "| Pencil|       716|2135.1400000000003|164.24153846153848|\n",
      "+-------+----------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .groupby('Item')\n",
    "    .agg(\n",
    "          f.sum('Units').alias('UnitsTotal')\n",
    "        , f.sum('Total').alias('GrandTotal')\n",
    "        , f.avg('Total').alias('AvgPerTransaction')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting data\n",
    "## .select(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|     Rep|  Total|\n",
      "+--------+-------+\n",
      "|   Jones| 189.05|\n",
      "|  Kivell|  999.5|\n",
      "| Jardine| 179.64|\n",
      "|    Gill| 539.73|\n",
      "| Sorvino| 167.44|\n",
      "|   Jones|  299.4|\n",
      "| Andrews| 149.25|\n",
      "| Jardine|  449.1|\n",
      "|Thompson|  63.68|\n",
      "|   Jones|  539.4|\n",
      "|  Morgan|  449.1|\n",
      "|  Howard|  57.71|\n",
      "|  Parent|1619.19|\n",
      "|   Jones| 174.65|\n",
      "|   Smith|  250.0|\n",
      "|   Jones| 255.84|\n",
      "|  Morgan| 251.72|\n",
      "|   Jones| 575.36|\n",
      "|  Parent| 299.85|\n",
      "|  Kivell| 479.04|\n",
      "+--------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .select('Rep','Total')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .sql(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+\n",
      "| OrderDate|    Rep| Region| Total|\n",
      "+----------+-------+-------+------+\n",
      "|2016-04-18|Andrews|Central|149.25|\n",
      "|2017-04-10|Andrews|Central|131.34|\n",
      "|2017-10-31|Andrews|Central| 18.06|\n",
      "|2017-12-21|Andrews|Central|139.72|\n",
      "+----------+-------+-------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT OrderDate\n",
    "        , Rep\n",
    "        , Region\n",
    "        , Total\n",
    "    FROM sample_df\n",
    "    ORDER BY Rep\n",
    "        , OrderDate\n",
    "''').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commission = spark.createDataFrame(\n",
    "    sc.parallelize([\n",
    "          ('Central', 0.033)\n",
    "        , ('East',    0.032)\n",
    "        , ('West',    0.034)\n",
    "    ])\n",
    "    , ['Region', 'Commission']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "| Region| OrderDate|    Rep|  Item|Units|UnitCost| Total|Commission|CommissionValue|\n",
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "|Central|      null| Kivell|Binder|   50|   19.99| 999.5|     0.033|           33.0|\n",
      "|Central|2016-02-09|Jardine|Pencil|   36|    4.99|179.64|     0.033|            6.0|\n",
      "|Central|2016-02-26|   Gill|   Pen|   27|   19.99|539.73|     0.033|           18.0|\n",
      "|Central|2016-04-18|Andrews|Pencil|   75|    1.99|149.25|     0.033|            5.0|\n",
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .join(commission, on=['Region'], how='left_outer')\n",
    "    .withColumn('CommissionValue', f.round(f.col('Total') * f.col('Commission')))\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "| Region|    Rep|\n",
      "+-------+-------+\n",
      "|   East|  Jones|\n",
      "|Central| Kivell|\n",
      "|Central|Jardine|\n",
      "|Central|   Gill|\n",
      "+-------+-------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .select('Region', 'Rep')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Region='East', Rep='Jones'), Row(Region='Central', Rep='Kivell'), Row(Region='Central', Rep='Jardine'), Row(Region='Central', Rep='Gill')]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .select('Region', 'Rep')\n",
    "    .take(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-04-18|Central|Andrews|Pencil|   75|    1.99|149.25|\n",
      "|2017-12-21|Central|Andrews|Binder|   28|    4.99|139.72|\n",
      "|2017-10-31|Central|Andrews|Pencil|   14|    1.29| 18.06|\n",
      "|2017-04-10|Central|Andrews|Pencil|   66|    1.99|131.34|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .orderBy('Rep', 'Region')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----+--------+------+\n",
      "| OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "|2016-04-18|Central|Andrews|Pencil|   75|    1.99|149.25|\n",
      "|2017-12-21|Central|Andrews|Binder|   28|    4.99|139.72|\n",
      "|2017-10-31|Central|Andrews|Pencil|   14|    1.29| 18.06|\n",
      "|2017-04-10|Central|Andrews|Pencil|   66|    1.99|131.34|\n",
      "+----------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .sort('Rep', 'Region')\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .csv('../data/sample_data_inferred.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .write\n",
    "    .parquet(\n",
    "        '../data/sample_data_inferred.parquet'\n",
    "        , mode='overwrite'\n",
    "        , partitionBy='Rep'\n",
    "        , compression='gzip'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .write\n",
    "    .json(\n",
    "        '../data/sample_data_inferred.json'\n",
    "        , mode='overwrite'\n",
    "        , dateFormat='yyyy-mm-dd'\n",
    "        , compression='gzip'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitfalls of using pure Python UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://raw.githubusercontent.com/drabastomek/learningPySpark_video/master/common/images/udf.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateCommission(value, commissionPercent):\n",
    "    return value * commissionPercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "| Region| OrderDate|    Rep|  Item|Units|UnitCost| Total|Commission|CommissionValue|\n",
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "|Central|      null| Kivell|Binder|   50|   19.99| 999.5|     0.033|        32.9835|\n",
      "|Central|2016-02-09|Jardine|Pencil|   36|    4.99|179.64|     0.033|        5.92812|\n",
      "|Central|2016-02-26|   Gill|   Pen|   27|   19.99|539.73|     0.033|       17.81109|\n",
      "|Central|2016-04-18|Andrews|Pencil|   75|    1.99|149.25|     0.033|        4.92525|\n",
      "+-------+----------+-------+------+-----+--------+------+----------+---------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_df_inferred\n",
    "    .join(commission, on=['Region'], how='left_outer')\n",
    "    .withColumn('CommissionValue', calculateCommission(f.col('Total'), f.col('Commission')))\n",
    "    .show(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "sample_df_repartitioned = sample_df_inferred.repartition(4, 'Rep')\n",
    "sample_df_repartitioned.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
